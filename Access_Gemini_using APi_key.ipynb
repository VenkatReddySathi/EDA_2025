{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3cefb-65ee-4b2e-8c1a-6469fb3dc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edd98ca-f7c7-4c8e-ac1c-fe84c1fdb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b404aec-5c12-4879-97a8-8c6caa144493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in e:\\anaconda\\lib\\site-packages (1.26.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in e:\\anaconda\\lib\\site-packages (from google-genai) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in e:\\anaconda\\lib\\site-packages (from google-genai) (2.38.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in e:\\anaconda\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in e:\\anaconda\\lib\\site-packages (from google-genai) (2.11.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in e:\\anaconda\\lib\\site-packages (from google-genai) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in e:\\anaconda\\lib\\site-packages (from google-genai) (8.2.3)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in e:\\anaconda\\lib\\site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in e:\\anaconda\\lib\\site-packages (from google-genai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\anaconda\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (2.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\anaconda\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in e:\\anaconda\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\anaconda\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in e:\\anaconda\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9)\n",
      "Requirement already satisfied: certifi in e:\\anaconda\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\anaconda\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\anaconda\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\anaconda\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\anaconda\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\anaconda\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.2.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in e:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "pip install -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a41ae46c-6bf3-4f76-b30a-860ffbea8114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in e:\\anaconda\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in e:\\anaconda\\lib\\site-packages (from google-genai) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in e:\\anaconda\\lib\\site-packages (from google-genai) (2.38.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in e:\\anaconda\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in e:\\anaconda\\lib\\site-packages (from google-genai) (2.11.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in e:\\anaconda\\lib\\site-packages (from google-genai) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in e:\\anaconda\\lib\\site-packages (from google-genai) (8.2.3)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in e:\\anaconda\\lib\\site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in e:\\anaconda\\lib\\site-packages (from google-genai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\anaconda\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (2.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\anaconda\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in e:\\anaconda\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in e:\\anaconda\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in e:\\anaconda\\lib\\site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9)\n",
      "Requirement already satisfied: certifi in e:\\anaconda\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\anaconda\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\anaconda\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\anaconda\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\anaconda\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\anaconda\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.2.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in e:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "# some times we might be use ! mark \n",
    "# when we install in jupyter notebook\n",
    "!pip install -U google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c88ddb-129b-41f6-bd71-fd0cda9ac060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sir after running this : from google import genai I am not getting any thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42ab48c4-f450-4299-be67-dc1a3c1146f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "483310f1-f26b-43c3-8afd-a65e6ea41187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb236265-6524-4a64-b8b0-487f3dd2088d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "randint(1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50f6739f-2416-404a-b74a-302150ff6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa1c0fbd-aa50-4d73-93f1-a5dfaf3f1183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Client',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_adapters',\n",
       " '_api_client',\n",
       " '_api_module',\n",
       " '_base_url',\n",
       " '_common',\n",
       " '_extra_utils',\n",
       " '_live_converters',\n",
       " '_mcp_utils',\n",
       " '_replay_api_client',\n",
       " '_tokens_converters',\n",
       " '_transformers',\n",
       " 'batches',\n",
       " 'caches',\n",
       " 'chats',\n",
       " 'client',\n",
       " 'errors',\n",
       " 'files',\n",
       " 'live',\n",
       " 'live_music',\n",
       " 'models',\n",
       " 'operations',\n",
       " 'pagers',\n",
       " 'tokens',\n",
       " 'tunings',\n",
       " 'types',\n",
       " 'version']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(genai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a45702b-4632-4431-a1c7-6a950b359109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be51353f-cb03-42fb-b61b-70e890b27320",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyDtcfGyNRfiHbgA5xlkxmEnO_-w34NqZEM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e0122f4-559c-4e03-bf63-c64cec71edd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object list_models at 0x0000022F37A4F970>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models=genai.list_models()\n",
    "list(models)  # int()  float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1faad04e-d5ad-4602-96be-ced52692e974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-vision-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
       "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
       "                    'Move to a newer Gemini version.'),\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/gemini-pro-vision',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
       "                    'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
       "                    'Move to a newer Gemini version.'),\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/gemini-1.5-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
       "                    'million tokens.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Gemini 1.5 Pro 002',\n",
       "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
       "                    'supports up to 2 million tokens, released in September of 2024.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro',\n",
       "       description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
       "                    'supports up to 2 million tokens, released in May of 2024.'),\n",
       "       input_token_limit=2000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
       "                    'across diverse tasks.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash',\n",
       "       description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
       "                    'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Gemini 1.5 Flash 002',\n",
       "       description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
       "                    'for scaling across diverse tasks, released in September of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B',\n",
       "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
       "                    'Flash model, released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B 001',\n",
       "       description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
       "                    'Flash model, released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-1.5-flash-8b-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash-8B Latest',\n",
       "       description=('Alias that points to the most recent production (non-experimental) release '\n",
       "                    'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
       "                    'released in October of 2024.'),\n",
       "       input_token_limit=1000000,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.5-pro-preview-03-25',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-03-25',\n",
       "       display_name='Gemini 2.5 Pro Preview 03-25',\n",
       "       description='Gemini 2.5 Pro Preview 03-25',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-05-20',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash',\n",
       "       description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
       "                    'supports up to 1 million tokens, released in June of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-lite-preview-06-17',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-06-17',\n",
       "       display_name='Gemini 2.5 Flash-Lite Preview 06-17',\n",
       "       description='Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-05-06',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-06',\n",
       "       display_name='Gemini 2.5 Pro Preview 05-06',\n",
       "       description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-06-05',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-06-05',\n",
       "       display_name='Gemini 2.5 Pro Preview',\n",
       "       description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro',\n",
       "       base_model_id='',\n",
       "       version='2.5',\n",
       "       display_name='Gemini 2.5 Pro',\n",
       "       description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-exp',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Experimental',\n",
       "       description='Gemini 2.0 Flash Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash',\n",
       "       description='Gemini 2.0 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash 001',\n",
       "       description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
       "                    'for scaling across diverse tasks, released in January of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite 001',\n",
       "       description='Stable version of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite',\n",
       "       description='Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-preview-image-generation',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Preview Image Generation',\n",
       "       description='Gemini 2.0 Flash Preview Image Generation',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-pro-exp',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini 2.0 Pro Experimental',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-pro-exp-02-05',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini 2.0 Pro Experimental 02-05',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-exp-1206',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-05-20',\n",
       "       display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "       description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Preview TTS',\n",
       "       description='Gemini 2.5 Flash Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Pro Preview TTS',\n",
       "       description='Gemini 2.5 Pro Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/learnlm-2.0-flash-experimental',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='LearnLM 2.0 Flash Experimental',\n",
       "       description='LearnLM 2.0 Flash Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-1b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 1B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 4B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-12b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 12B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-27b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 27B',\n",
       "       description='',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E4B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e2b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E2B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/text-embedding-004',\n",
       "       base_model_id='',\n",
       "       version='004',\n",
       "       display_name='Text Embedding 004',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp-03-07',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental 03-07',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=40),\n",
       " Model(name='models/imagen-3.0-generate-002',\n",
       "       base_model_id='',\n",
       "       version='002',\n",
       "       display_name='Imagen 3.0 002 model',\n",
       "       description='Vertex served Imagen 3.0 002 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 (Preview)',\n",
       "       description='Vertex served Imagen 4.0 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 Ultra (Preview)',\n",
       "       description='Vertex served Imagen 4.0 ultra model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-2.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Veo 2',\n",
       "       description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
       "                    'enabled on the associated Google Cloud Platform account. Please visit '\n",
       "                    'https://console.cloud.google.com/billing to enable it.'),\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.0-generate-preview',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Veo 3',\n",
       "       description=('Veo 3 preview. Access to this model requires billing to be enabled on the '\n",
       "                    'associated Google Cloud Platform account. Please visit '\n",
       "                    'https://console.cloud.google.com/billing to enable it.'),\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-2.5-flash-preview-native-audio-dialog',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
       "       description='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-exp-native-audio-thinking-dialog',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
       "       description='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-live-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.0 Flash 001',\n",
       "       description='Gemini 2.0 Flash 001',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-live-2.5-flash-preview',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini Live 2.5 Flash Preview',\n",
       "       description='Gemini Live 2.5 Flash Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-live-preview',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash Live Preview',\n",
       "       description='Gemini 2.5 Flash Live Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyDtcfGyNRfiHbgA5xlkxmEnO_-w34NqZEM\")\n",
    "models=genai.list_models()\n",
    "list(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6a47492-5769-4275-9767-090f1dde1442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-2.5-flash',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=genai.GenerativeModel('gemini-2.5-flash')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0d9427c-48df-4d81-bcba-ed9ba49ad4dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Quota exceeded for quota metric 'Generate Content API requests per minute' and limit 'GenerateContent request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:201395404203'. [reason: \"RATE_LIMIT_EXCEEDED\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\nmetadata {\n  key: \"quota_unit\"\n  value: \"1/min/{project}/{region}\"\n}\nmetadata {\n  key: \"quota_metric\"\n  value: \"generativelanguage.googleapis.com/generate_content_requests\"\n}\nmetadata {\n  key: \"quota_location\"\n  value: \"us-south1\"\n}\nmetadata {\n  key: \"quota_limit\"\n  value: \"GenerateContentRequestsPerMinutePerProjectPerRegion\"\n}\nmetadata {\n  key: \"quota_limit_value\"\n  value: \"0\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/201395404203\"\n}\n, links {\n  description: \"Request a higher quota limit.\"\n  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate_content(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwho are the top 5 crickters in India\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m response\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m    332\u001b[0m             request,\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options,\n\u001b[0;32m    334\u001b[0m         )\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:868\u001b[0m, in \u001b[0;36mgenerate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_answer\u001b[39m(\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    847\u001b[0m     request: Optional[Union[generative_service\u001b[38;5;241m.\u001b[39mGenerateAnswerRequest, \u001b[38;5;28mdict\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m     metadata: Sequence[Tuple[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m]]] \u001b[38;5;241m=\u001b[39m (),\n\u001b[0;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m generative_service\u001b[38;5;241m.\u001b[39mGenerateAnswerResponse:\n\u001b[0;32m    859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Generates a grounded answer from the model given an input\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m    ``GenerateAnswerRequest``.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03m    .. code-block:: python\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \n\u001b[0;32m    864\u001b[0m \u001b[38;5;124;03m        # This snippet has been automatically generated and should be regarded as a\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;124;03m        # code template only.\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;124;03m        # It will require modifications to work:\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m        # - It may require correct/in-range values for request initialization.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m \u001b[38;5;124;03m        # - It may require specifying regional endpoints when creating the service\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;124;03m        #   client as shown in:\u001b[39;00m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m        #   https://googleapis.dev/python/google-api-core/latest/client_options.html\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m        from google.ai import generativelanguage_v1beta\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m        def sample_generate_answer():\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;124;03m            # Create a client\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;124;03m            client = generativelanguage_v1beta.GenerativeServiceClient()\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m            # Initialize request argument(s)\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m            request = generativelanguage_v1beta.GenerateAnswerRequest(\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m                model=\"model_value\",\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m                answer_style=\"VERBOSE\",\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;124;03m            )\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \n\u001b[0;32m    883\u001b[0m \u001b[38;5;124;03m            # Make the request\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;124;03m            response = client.generate_answer(request=request)\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m            # Handle the response\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m            print(response)\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \n\u001b[0;32m    889\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m        request (Union[google.ai.generativelanguage_v1beta.types.GenerateAnswerRequest, dict]):\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;124;03m            The request object. Request to generate a grounded answer from the\u001b[39;00m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m            ``Model``.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m        model (str):\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m            Required. The name of the ``Model`` to use for\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;124;03m            generating the grounded response.\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \n\u001b[0;32m    897\u001b[0m \u001b[38;5;124;03m            Format: ``model=models/{model}``.\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \n\u001b[0;32m    899\u001b[0m \u001b[38;5;124;03m            This corresponds to the ``model`` field\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03m            on the ``request`` instance; if ``request`` is provided, this\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;124;03m            should not be set.\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m        contents (MutableSequence[google.ai.generativelanguage_v1beta.types.Content]):\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m            Required. The content of the current conversation with\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m            the ``Model``. For single-turn queries, this is a single\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;124;03m            question to answer. For multi-turn queries, this is a\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;124;03m            repeated field that contains conversation history and\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;124;03m            the last ``Content`` in the list containing the\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m            question.\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m            Note: ``GenerateAnswer`` only supports queries in\u001b[39;00m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m            English.\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \n\u001b[0;32m    913\u001b[0m \u001b[38;5;124;03m            This corresponds to the ``contents`` field\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m            on the ``request`` instance; if ``request`` is provided, this\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m            should not be set.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03m        safety_settings (MutableSequence[google.ai.generativelanguage_v1beta.types.SafetySetting]):\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;124;03m            Optional. A list of unique ``SafetySetting`` instances\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03m            for blocking unsafe content.\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m            This will be enforced on the\u001b[39;00m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;124;03m            ``GenerateAnswerRequest.contents`` and\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;124;03m            ``GenerateAnswerResponse.candidate``. There should not\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;124;03m            be more than one setting for each ``SafetyCategory``\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;124;03m            type. The API will block any contents and responses that\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;124;03m            fail to meet the thresholds set by these settings. This\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;124;03m            list overrides the default settings for each\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m            ``SafetyCategory`` specified in the safety_settings. If\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m            there is no ``SafetySetting`` for a given\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m            ``SafetyCategory`` provided in the list, the API will\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;124;03m            use the default safety setting for that category. Harm\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03m            categories HARM_CATEGORY_HATE_SPEECH,\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m            HARM_CATEGORY_SEXUALLY_EXPLICIT,\u001b[39;00m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;124;03m            HARM_CATEGORY_DANGEROUS_CONTENT,\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m            HARM_CATEGORY_HARASSMENT are supported. Refer to the\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m            `guide <https://ai.google.dev/gemini-api/docs/safety-settings>`__\u001b[39;00m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;124;03m            for detailed information on available safety settings.\u001b[39;00m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;124;03m            Also refer to the `Safety\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03m            guidance <https://ai.google.dev/gemini-api/docs/safety-guidance>`__\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;124;03m            to learn how to incorporate safety considerations in\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;124;03m            your AI applications.\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[38;5;124;03m            This corresponds to the ``safety_settings`` field\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m            on the ``request`` instance; if ``request`` is provided, this\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m            should not be set.\u001b[39;00m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m        answer_style (google.ai.generativelanguage_v1beta.types.GenerateAnswerRequest.AnswerStyle):\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m            Required. Style in which answers\u001b[39;00m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;124;03m            should be returned.\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \n\u001b[0;32m    949\u001b[0m \u001b[38;5;124;03m            This corresponds to the ``answer_style`` field\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;124;03m            on the ``request`` instance; if ``request`` is provided, this\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;124;03m            should not be set.\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;124;03m        retry (google.api_core.retry.Retry): Designation of what errors, if any,\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;124;03m            should be retried.\u001b[39;00m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;124;03m        timeout (float): The timeout for this request.\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;124;03m        metadata (Sequence[Tuple[str, Union[str, bytes]]]): Key/value pairs which should be\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m            sent along with the request as metadata. Normally, each value must be of type `str`,\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m            but for metadata keys ending with the suffix `-bin`, the corresponding values must\u001b[39;00m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;124;03m            be of type `bytes`.\u001b[39;00m\n\u001b[0;32m    959\u001b[0m \n\u001b[0;32m    960\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;124;03m        google.ai.generativelanguage_v1beta.types.GenerateAnswerResponse:\u001b[39;00m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;124;03m            Response from the model for a\u001b[39;00m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;124;03m            grounded answer.\u001b[39;00m\n\u001b[0;32m    964\u001b[0m \n\u001b[0;32m    965\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;66;03m# Create or coerce a protobuf request object.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;66;03m# - Quick check: If we got a request object, we should *not* have\u001b[39;00m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m#   gotten any keyword arguments that map to the request.\u001b[39;00m\n\u001b[0;32m    969\u001b[0m     has_flattened_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m([model, contents, safety_settings, answer_style])\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    294\u001b[0m     target,\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    296\u001b[0m     sleep_generator,\n\u001b[0;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     _retry_error_helper(\n\u001b[0;32m    154\u001b[0m         exc,\n\u001b[0;32m    155\u001b[0m         deadline,\n\u001b[0;32m    156\u001b[0m         sleep,\n\u001b[0;32m    157\u001b[0m         error_list,\n\u001b[0;32m    158\u001b[0m         predicate,\n\u001b[0;32m    159\u001b[0m         on_error,\n\u001b[0;32m    160\u001b[0m         exception_factory,\n\u001b[0;32m    161\u001b[0m         timeout,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 Quota exceeded for quota metric 'Generate Content API requests per minute' and limit 'GenerateContent request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:201395404203'. [reason: \"RATE_LIMIT_EXCEEDED\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\nmetadata {\n  key: \"quota_unit\"\n  value: \"1/min/{project}/{region}\"\n}\nmetadata {\n  key: \"quota_metric\"\n  value: \"generativelanguage.googleapis.com/generate_content_requests\"\n}\nmetadata {\n  key: \"quota_location\"\n  value: \"us-south1\"\n}\nmetadata {\n  key: \"quota_limit\"\n  value: \"GenerateContentRequestsPerMinutePerProjectPerRegion\"\n}\nmetadata {\n  key: \"quota_limit_value\"\n  value: \"0\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/201395404203\"\n}\n, links {\n  description: \"Request a higher quota limit.\"\n  url: \"https://cloud.google.com/docs/quotas/help/request_increase\"\n}\n]"
     ]
    }
   ],
   "source": [
    "response=model.generate_content('who are the top 5 crickters in India')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c49a28b-4451-4447-b8df-296d40a185e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Defining the \\\"top 5\\\" cricketers is always subjective, as it can depend on current form, historical performance, format specialisation (Test, ODI, T20), and impact on the team.\\n\\nHowever, based on consistent performance, impact, and overall stature in recent times, here are 5 cricketers who are widely considered among India's best:\\n\\n1.  **Virat Kohli:** Still one of the world's leading batsmen, particularly in ODIs and Tests. His incredible consistency, record-breaking achievements, and match-winning abilities make him a perennial top choice.\\n2.  **Rohit Sharma:** The captain of the Indian team across formats, known for his elegant strokeplay, massive hundreds in ODIs, and crucial contributions as an opener in Tests. His leadership and big-match temperament are invaluable.\\n3.  **Jasprit Bumrah:** Arguably the best all-format fast bowler in the world. His unique action, accuracy, pace, and ability to bowl devastating Yorkers and bouncers make him a nightmare for batsmen in any situation.\\n4.  **Ravindra Jadeja:** A true multi-format all-rounder. His left-arm spin is highly effective, his aggressive lower-order batting has won many games, and his fielding is arguably the best in the world, making him an indispensable asset.\\n5.  **Shubman Gill:** Represents the exciting new generation. He has rapidly established himself as a dominant force across all formats with his elegant batting, high scores, and immense potential, making him a cornerstone of India's future.\\n\\n**Notable Mentions (who could easily be in the top 5 depending on the day/criteria):**\\n\\n*   **Mohammed Shami:** A fantastic seamer, especially lethal in ODIs and Tests with his ability to pick up crucial wickets and provide breakthroughs.\\n*   **Hardik Pandya:** When fit and firing, he's a game-changing all-rounder with his powerful hitting and medium-pace bowling, particularly in white-ball cricket.\\n*   **Rishabh Pant:** Before his injury, he was one of the most exciting and impactful Test wicketkeeper-batsmen in the world, capable of turning games single-handedly. His return is highly anticipated.\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 11,\n",
       "        \"candidates_token_count\": 466,\n",
       "        \"total_token_count\": 1487,\n",
       "        \"prompt_tokens_details\": [\n",
       "          {\n",
       "            \"modality\": \"TEXT\",\n",
       "            \"token_count\": 11\n",
       "          }\n",
       "        ],\n",
       "        \"thoughts_token_count\": 1010\n",
       "      },\n",
       "      \"model_version\": \"gemini-2.5-flash\"\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyAuZyG_6GSDDO_iYTM1XrHfF4LMMWzryUA\")\n",
    "model=genai.GenerativeModel('gemini-2.5-flash')\n",
    "response=model.generate_content('who are the top 5 crickters in India')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77aeec33-de3d-4ef8-8917-c8a0913c6a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the \"top 5\" cricketers is always subjective, as it can depend on current form, historical performance, format specialisation (Test, ODI, T20), and impact on the team.\n",
      "\n",
      "However, based on consistent performance, impact, and overall stature in recent times, here are 5 cricketers who are widely considered among India's best:\n",
      "\n",
      "1.  **Virat Kohli:** Still one of the world's leading batsmen, particularly in ODIs and Tests. His incredible consistency, record-breaking achievements, and match-winning abilities make him a perennial top choice.\n",
      "2.  **Rohit Sharma:** The captain of the Indian team across formats, known for his elegant strokeplay, massive hundreds in ODIs, and crucial contributions as an opener in Tests. His leadership and big-match temperament are invaluable.\n",
      "3.  **Jasprit Bumrah:** Arguably the best all-format fast bowler in the world. His unique action, accuracy, pace, and ability to bowl devastating Yorkers and bouncers make him a nightmare for batsmen in any situation.\n",
      "4.  **Ravindra Jadeja:** A true multi-format all-rounder. His left-arm spin is highly effective, his aggressive lower-order batting has won many games, and his fielding is arguably the best in the world, making him an indispensable asset.\n",
      "5.  **Shubman Gill:** Represents the exciting new generation. He has rapidly established himself as a dominant force across all formats with his elegant batting, high scores, and immense potential, making him a cornerstone of India's future.\n",
      "\n",
      "**Notable Mentions (who could easily be in the top 5 depending on the day/criteria):**\n",
      "\n",
      "*   **Mohammed Shami:** A fantastic seamer, especially lethal in ODIs and Tests with his ability to pick up crucial wickets and provide breakthroughs.\n",
      "*   **Hardik Pandya:** When fit and firing, he's a game-changing all-rounder with his powerful hitting and medium-pace bowling, particularly in white-ball cricket.\n",
      "*   **Rishabh Pant:** Before his injury, he was one of the most exciting and impactful Test wicketkeeper-batsmen in the world, capable of turning games single-handedly. His return is highly anticipated.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3897fe8a-e74a-4541-ac74-4e28ec43dc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinpointing the \"top 5\" Indian cricketers can be subjective and depend on whether you're talking about current form, all-time greatness, or a mix of both. However, considering a blend of recent performance, impact, and overall career significance among active and recently retired legends, here's a widely accepted list:\n",
      "\n",
      "**Considering a mix of current form and recent legends:**\n",
      "\n",
      "1.  **Virat Kohli:** Often considered one of the greatest batsmen of all time, especially in white-ball cricket. His consistency, ability to score centuries, and master of the chase make him an undisputed legend and a current force.\n",
      "2.  **Rohit Sharma:** The current captain across formats, known for his elegant strokeplay and prolific scoring, particularly in white-ball cricket where he holds multiple records for double centuries in ODIs and centuries in T20Is. He has also established himself as a solid Test opener.\n",
      "3.  **Jasprit Bumrah:** A world-class fast bowler across all formats. His unique action, searing pace, deadly yorkers, and ability to pick up wickets at crucial junctures make him one of the most feared bowlers globally.\n",
      "4.  **Ravindra Jadeja:** A true all-rounder who excels in all three departments. His left-arm spin is penetrative, his lower-order batting has won many matches, and his fielding is arguably among the best in the world.\n",
      "5.  **MS Dhoni:** While retired from international cricket, his impact as a captain (leading India to all three ICC limited-overs trophies) and as one of the greatest finishers in white-ball cricket is undeniable. He remains a huge figure in Indian cricket.\n",
      "\n",
      "**If strictly considering active players based on current form and impact, a strong contender for the 5th spot would be:**\n",
      "\n",
      "*   **Mohammed Shami:** A fantastic fast bowler, especially potent with the new ball and in the death overs. His recent performances in ODIs and Tests have been exceptional, showcasing incredible skill and wicket-taking ability.\n",
      "\n",
      "**All-Time Legends (Beyond the scope of \"current top 5\" but essential to mention):**\n",
      "\n",
      "*   **Sachin Tendulkar:** Widely regarded as the \"God of Cricket,\" holding almost every major batting record.\n",
      "*   **Kapil Dev:** India's greatest all-rounder and the first captain to win a World Cup (1983).\n",
      "*   **Rahul Dravid:** \"The Wall,\" known for his resilience and solid technique in Test cricket.\n",
      "*   **Sunil Gavaskar:** The first batsman to score 10,000 Test runs, a legend of opening batting.\n",
      "\n",
      "The list above focuses on modern-era players who have significantly shaped Indian cricket over the last decade or so.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyAuZyG_6GSDDO_iYTM1XrHfF4LMMWzryUA\")\n",
    "model=genai.GenerativeModel('gemini-2.5-flash')\n",
    "response=model.generate_content('who are the top 5 crickters in India')              \n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdcabe92-f273-4b57-85e8-10b5cfefabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error1\n",
    "# when i ran ppl are saying im not getting output\n",
    "# pacakges will not give the answer\n",
    "# if you want to see any answer we need print\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b0e8d4-6098-403a-a216-0c85e8f821c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error2\n",
    "a=100\n",
    "b=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b8234c-7497-4851-957d-0bdf3ffbbec6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a\u001b[38;5;241m+\u001b[39mb\n",
      "\u001b[1;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53dba3-8ddd-430f-a858-da9acbef2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "# package error\n",
    "# installed the packed\n",
    "# you should run the line again\n",
    "# that you are not doing\n",
    "\n",
    "\n",
    "# once install package rerun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314301ff-670d-469f-a9b3-0510afcc5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error-3\n",
    "genai.configure(api_key=\"<write something>\")\n",
    "\n",
    "# dont include <>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06520f2b-9275-40f0-9939-4d3b6e22f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error-4\n",
    "genai.configure(\"\")\n",
    "# variable name mention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7488ff-4c25-4365-bb2a-6a35beb530d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 5: model name error\n",
    "#          proper name provide\n",
    "\n",
    "# Error-6: give the varaibename model\n",
    "model=genai.GenerativeModel('gemini-2.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662208f-02cf-4642-b93d-65e59d411518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  error7:generate_content\n",
    "#         GenerativeModel\n",
    "response=model.generate_content('who are the top 5 crickters in India')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0381b35e-57e7-428e-a8f7-2dbe6c62d295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response.text\n"
     ]
    }
   ],
   "source": [
    "# Error-8:\n",
    "print(\"response.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a743d9-2456-44a3-b3f7-a98aa1ecbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# its a learning\n",
    "# you are starting\n",
    "# do the mistakes\n",
    "# every mistake i will help \n",
    "# but dont do next time\n",
    "\n",
    "# New mistakes \n",
    "# dont do old mistakes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
